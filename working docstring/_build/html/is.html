<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Superset Behaviour &#8212; tr  documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css" />
    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/sphinx_highlight.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="superset-behaviour">
<h1>Superset Behaviour<a class="headerlink" href="#superset-behaviour" title="Permalink to this heading">¶</a></h1>
<p>When implementing functions in Ivy, whether they are primary, compositional or mixed, we are constantly faced with the question: which backend implementation should Ivy most closely follow?</p>
<section id="extending-the-standard">
<h2>Extending the Standard<a class="headerlink" href="#extending-the-standard" title="Permalink to this heading">¶</a></h2>
<p>It might seem as though this question is already answered.
Ivy fully adheres to the <a class="reference external" href="https://data-apis.org/array-api/latest/">Array API Standard</a>, which helpfully limits our design space for the functions, but in its current form this only covers a relatively small number of functions, which together make up less than half of the functions in Ivy.
Even for Ivy functions which adhere to the standard, the standard permits the addition of extra arguments and function features, provided that they do not contradict the requirements of the standard.
Therefore, we are still faced with the same kind of design decisions for all Ivy functions, even those appearing in the <a class="reference external" href="https://data-apis.org/array-api/latest/">Array API Standard</a>.</p>
</section>
<section id="what-is-the-superset">
<h2>What is the Superset?<a class="headerlink" href="#what-is-the-superset" title="Permalink to this heading">¶</a></h2>
<p>We explain through examples how Ivy always goes for the superset of functionality among the backend frameworks.
This means that even if only one framework supports a certain feature, then we still strive to include this feature in the Ivy function.
The Ivy function then entails the <em>superset</em> of all backend features.
However, this is not always totally possible, and in some cases certain framework-specific features must be sacrificed, but usually it’s possible to implement a very generalized function which covers most of the unique features among the corresponding functions in each framework.</p>
<p>We strive to implement the superset for primary, compositional and mixed functions.
In many cases compositional functions do not actually have corresponding backend-specific functions, but this is not always the case.
For example, <code class="xref py py-func docutils literal notranslate"><span class="pre">ivy.linear()</span></code> is a fully compositional function, but <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.functional.linear()</span></code> also exists.
We should therefore make sure the compositional <code class="xref py py-func docutils literal notranslate"><span class="pre">ivy.linear()</span></code> function includes all behaviours supported by <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.functional.linear()</span></code>.</p>
</section>
<section id="a-non-duplicate-superset">
<h2>A Non-Duplicate Superset<a class="headerlink" href="#a-non-duplicate-superset" title="Permalink to this heading">¶</a></h2>
<p>It would be easy to assume that implementing the superset simply means adding all arguments from all related functions into the Ivy function.
However, this is <strong>not</strong> the case for a few reasons.
Firstly, different functions might have different argument names for the same behaviour.
Looking at the functions <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html">numpy.concatenate</a> and <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.cat.html">torch.cat</a>, we of course do not want to add both of the arguments <code class="code docutils literal notranslate"><span class="pre">axis</span></code> and <code class="code docutils literal notranslate"><span class="pre">dim</span></code> to <code class="xref py py-func docutils literal notranslate"><span class="pre">ivy.concat()</span></code>, as these both represent exactly the same thing: the dimemsion/axis along which to concatenate.
In this case, the argument is <a class="reference external" href="https://data-apis.org/array-api/latest/API_specification/generated/signatures.manipulation_functions.concat.html">covered</a> in the <a class="reference external" href="https://data-apis.org/array-api/latest/">Array API Standard</a> and so we opt for <code class="code docutils literal notranslate"><span class="pre">axis</span></code>.
In cases where there are differences between the backend argument names, and the function or argument is not in the standard, then it is up to us to determine which argument name to use.</p>
</section>
<section id="what-is-not-the-superset">
<h2>What is not the Superset?<a class="headerlink" href="#what-is-not-the-superset" title="Permalink to this heading">¶</a></h2>
<p>We’ve already explained that we should not duplicate arguments in the Ivy function when striving for the superset.
Does this mean, provided that the proposed argument is not a duplicate, that we should always add this backend-specific argument to the Ivy function?
The answer is <strong>no</strong>.
When determining the superset, we are only concerned with the pure <strong>mathematics</strong> of the function, and nothing else.
For example, the <code class="code docutils literal notranslate"><span class="pre">name</span></code> argument is common to many TensorFlow functions, such as <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/concat">tf.concat</a>, and is used for uniquely identifying parts of the compiled computation graph during logging and debugging.
This has nothing to do with the mathematics of the function, and so is <em>not</em> included in the superset considerations when implementing Ivy functions.
Similarly, in NumPy the argument <code class="code docutils literal notranslate"><span class="pre">subok</span></code> controls whether subclasses of the <code class="xref py py-class docutils literal notranslate"><span class="pre">numpy.ndarray</span></code> class should be permitted, which is included in many functions, such as <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.astype.html">numpy.ndarray.astype</a>.
Finally, in JAX the argument <code class="code docutils literal notranslate"><span class="pre">precision</span></code> is quite common, which controls the precision of the return values, as used in <a class="reference external" href="https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.conv.html">jax.lax.conv</a> for example.
Similarly, the functions <code class="xref py py-func docutils literal notranslate"><span class="pre">jacfwd()</span></code> and <code class="xref py py-func docutils literal notranslate"><span class="pre">jacrev()</span></code> in JAX are actually mathematically identical, and these functions differ <em>only</em> in their underlying algorithm, either forward mode or reverse mode.</p>
<p>None of the above arguments or function variants are included in our superset considerations, as again they are not relating to the pure mathematics, and instead relate to framework, hardware or algorithmic specifics.
Given the abstraction layer that Ivy operates at, Ivy is fundamentally unable to control under-the-hood specifics such as those mentioned above.
However, this is by design, and the central benefit of Ivy is the ability to abstract many different runtimes and algorithms under the same banner, unified by their shared fundamental mathematics.</p>
<p>A special case is the NumPy <code class="code docutils literal notranslate"><span class="pre">order</span></code> argument which controls the low-level memory layout of the array.
Although it normally has no effect on the mathematics of a function, in certain manipulation routines like <code class="code docutils literal notranslate"><span class="pre">reshape</span></code>, <code class="code docutils literal notranslate"><span class="pre">flatten</span></code> and <code class="code docutils literal notranslate"><span class="pre">ravel</span></code>, order determines the way the elements are read and placed into the reshaped array.
Therefore, ivy supports <code class="code docutils literal notranslate"><span class="pre">order</span></code> for these functions and any remaining logic surrounding order is handled in the NumPy frontend.</p>
<p>Regarding the <strong>only mathematics</strong> rule regarding the superset considerations, there are two exceptions to this, which are the handling of data type and device arguments.
Neither of these relate to the pure mathematics of the function.
However, as is discussed below, we always strive to implement Ivy functions such that they support as many data types and devices as possible.</p>
</section>
<section id="when-the-superset-is-too-much">
<h2>When the Superset is Too Much<a class="headerlink" href="#when-the-superset-is-too-much" title="Permalink to this heading">¶</a></h2>
<p>Despite this general approach, the total superset is not always actually strived for, especially in cases where the behaviour can very easily be replicated by a simple composition of other functions, or where the extra behaviour is redundant as it is already covered by another function.</p>
<p>As an example, many pointwise functions in NumPy support the <code class="code docutils literal notranslate"><span class="pre">where</span></code> argument, which enables a mask array to be specified, with the function then only evaluated at elements for which the <code class="code docutils literal notranslate"><span class="pre">where</span></code> array is <code class="code docutils literal notranslate"><span class="pre">True</span></code>.
This inclusion of this feature in NumPy is totally understandable, compositions of NumPy functions are never compiled into computation graphs which span multiple operations, and therefore a good way to maximize efficiency of NumPy code is to minimize the number of unique NumPy functions which are called, each of which are implemented with very high efficiency in <code class="code docutils literal notranslate"><span class="pre">C</span></code>.
In this case, the inclusion of <code class="code docutils literal notranslate"><span class="pre">where</span></code> as an argument also prevents unnecessary values from being computed in the first place, only to then be masked out in the immediately subsequent operation.
For these reasons, calling <code class="code docutils literal notranslate"><span class="pre">np.absolute(x,</span> <span class="pre">where=mask)</span></code> is much more efficient than calling <code class="code docutils literal notranslate"><span class="pre">np.where(mask,</span> <span class="pre">np.absolute(x),</span> <span class="pre">np.empty_like(x))</span></code> in NumPy.
<code class="xref py py-func docutils literal notranslate"><span class="pre">ivy.logical_and()</span></code> is another example where the superset is too much, as we explain in the extra examples given at the end of this section.</p>
<p>However, other frameworks are able to compile compositions of python operations directly to computation graphs in low-level languages, and are also able to intelligently fuse operations into combined kernels, via libraries such as <a class="reference external" href="https://github.com/NVIDIA/TensorRT">TensorRT</a>.
This removes the need for highly general function signatures such as those found in NumPy.
Instead, a compositional approach is preferred, where each function in Python generally serves a particular and non-overlapping purpose.
This helps to keep things more clean and clear at the Python level, without sacrificing efficiency at the lower level.</p>
</section>
<section id="balancing-generalization-with-efficiency">
<h2>Balancing Generalization with Efficiency<a class="headerlink" href="#balancing-generalization-with-efficiency" title="Permalink to this heading">¶</a></h2>
<p>Sometimes, the simplest way to implement superset behaviour comes at the direct expense of runtime efficiency.
We explore this through the examples of <code class="xref py py-func docutils literal notranslate"><span class="pre">softplus()</span></code>.</p>
<p><strong>ivy.softplus</strong></p>
<p>When looking at the <code class="xref py py-func docutils literal notranslate"><span class="pre">softplus()</span></code> (or closest equivalent) implementations for <a class="reference external" href="https://unify.ai/docs/ivy/functional/ivy/activations/softplus/softplus_functional.html">Ivy</a>, <a class="reference external" href="https://jax.readthedocs.io/en/latest/_autosummary/jax.nn.softplus.html">JAX</a>, <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/math/softplus">TensorFlow</a>, and <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.functional.softplus.html">PyTorch</a>, we can see that torch is the only framework which supports the inclusion of the <code class="code docutils literal notranslate"><span class="pre">beta</span></code> and <code class="code docutils literal notranslate"><span class="pre">threshold</span></code> arguments, which are added for improved numerical stability.
We can also see that numpy does not support a <code class="xref py py-func docutils literal notranslate"><span class="pre">softplus()</span></code> function at all.
Ivy should also support the <code class="code docutils literal notranslate"><span class="pre">beta</span></code> and <code class="code docutils literal notranslate"><span class="pre">threshold</span></code> arguments, in order to provide the generalized superset implementation among the backend frameworks.</p>
<p>Let’s take the tensorflow backend implementation as an example when assessing the necessary changes.
Without superset behaviour, the implementation is incredibly simple, with only a single tensorflow function called under the hood.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">softplus</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="o">/</span><span class="p">,</span>
             <span class="o">*</span><span class="p">,</span>
             <span class="n">out</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<p>The simplest approach would be to implement <code class="xref py py-func docutils literal notranslate"><span class="pre">softplus()</span></code> in each Ivy backend as a simple composition.
For example, a simple composition in the tensorflow backend would look like the following:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">softplus</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="o">/</span><span class="p">,</span>
             <span class="o">*</span><span class="p">,</span>
             <span class="n">beta</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
             <span class="n">threshold</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
             <span class="n">out</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="n">res</span> <span class="o">=</span> <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="n">beta</span><span class="p">))</span> <span class="o">/</span> <span class="n">beta</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="n">beta</span> <span class="o">&gt;</span> <span class="n">threshold</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">res</span><span class="p">)</span>
</pre></div>
</div>
<p>This approach uses the default argument values used by PyTorch, and it does indeed extend the behaviour correctly.
However, the implementation now uses <strong>six</strong> tensorflow function calls instead of one, being: <code class="xref py py-func docutils literal notranslate"><span class="pre">__mul__()</span></code>, <code class="xref py py-func docutils literal notranslate"><span class="pre">tf.nn.softplus()</span></code>, <code class="xref py py-func docutils literal notranslate"><span class="pre">__div__()</span></code>, <code class="xref py py-func docutils literal notranslate"><span class="pre">__mul__()</span></code>, <code class="xref py py-func docutils literal notranslate"><span class="pre">__gt__()</span></code>, <code class="xref py py-func docutils literal notranslate"><span class="pre">tf.where()</span></code> in order of execution.
If a user doesn’t care about the extra <code class="code docutils literal notranslate"><span class="pre">threshold</span></code> and <code class="code docutils literal notranslate"><span class="pre">beta</span></code> arguments, then a 6× increase in backend functions is a heavy price to pay efficiency-wise.</p>
<p>Therefore, we should in general adopt a different approach when implementing superset behaviour.
We should still implement the superset, but keep this extended behaviour as optional as possible, with maximal efficiency and minimal intrusion in the case that this extended behaviour is not required.
The following would be a much better solution:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">softplus</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="o">/</span><span class="p">,</span>
             <span class="o">*</span><span class="p">,</span>
             <span class="n">beta</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
             <span class="n">threshold</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
             <span class="n">out</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">beta</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">beta</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">x_beta</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">beta</span>
        <span class="n">res</span> <span class="o">=</span> <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span><span class="n">x_beta</span><span class="p">))</span> <span class="o">/</span> <span class="n">beta</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">x_beta</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">threshold</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">x_beta</span> <span class="o">&gt;</span> <span class="n">threshold</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">res</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">res</span>
</pre></div>
</div>
<p>You will notice that this implementation involves more lines of code, but this should not be confused with added complexity.
All Ivy code should be graph compiled for efficiency, and in this case all the <code class="code docutils literal notranslate"><span class="pre">if</span></code> and <code class="code docutils literal notranslate"><span class="pre">else</span></code> statements are removed, and all that remains is the backend functions which were executed.
This new implementation will be compiled to a graph of either one, three, four or six functions depending on the values of <code class="code docutils literal notranslate"><span class="pre">beta</span></code> and <code class="code docutils literal notranslate"><span class="pre">threshold</span></code>, while the previous implementation would <em>always</em> compile to six functions.</p>
<p>This does mean we do not adopt the default values used by PyTorch, but that’s okay.
Implementing the superset does not mean adopting the same default values for arguments, it simply means equipping the Ivy function with the capabilities to execute the superset of behaviours.</p>
</section>
<section id="more-examples">
<h2>More Examples<a class="headerlink" href="#more-examples" title="Permalink to this heading">¶</a></h2>
<p>We now take a look at some examples, and explain our rational for deciding upon the function signature that we should use in Ivy.
The first three examples are more-or-less superset examples, while the last example involves a deliberate decision to not implement the full superset, for some of the reasons explained above.</p>
<p><strong>ivy.linspace</strong></p>
<p>When looking at the <code class="xref py py-func docutils literal notranslate"><span class="pre">linspace()</span></code> (or closest equivalent) implementations for <a class="reference external" href="https://unify.ai/docs/ivy/functional/ivy/creation/linspace/linspace_functional.html">Ivy</a>, <a class="reference external" href="https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.linspace.html">JAX</a>, <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.linspace.html">NumPy</a>, <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/linspace">TensorFlow</a>, and <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.linspace.html">PyTorch</a>, we can see that torch does not support arrays for the <code class="code docutils literal notranslate"><span class="pre">start</span></code> and <code class="code docutils literal notranslate"><span class="pre">end</span></code> arguments, while JAX, numpy and tensorflow all do.
Likewise, Ivy also supports arrays for the <code class="code docutils literal notranslate"><span class="pre">start</span></code> and <code class="code docutils literal notranslate"><span class="pre">stop</span></code> arguments, and in doing so provides the generalized superset implementation among the backend frameworks.</p>
<p><strong>ivy.eye</strong></p>
<p>When looking at the <code class="xref py py-func docutils literal notranslate"><span class="pre">eye()</span></code> (or closest equivalent) implementations for <a class="reference external" href="https://unify.ai/docs/ivy/functional/ivy/creation/eye/eye_functional.html">Ivy</a>, <a class="reference external" href="https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.eye.html">JAX</a>, <a class="reference external" href="https://numpy.org/devdocs/reference/generated/numpy.eye.html">NumPy</a>, <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/eye">TensorFlow</a>, and <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.eye.html">PyTorch</a>, we can see that tensorflow is the only framework which supports a <code class="code docutils literal notranslate"><span class="pre">batch_shape</span></code> argument.
Likewise, Ivy also supports a <code class="code docutils literal notranslate"><span class="pre">batch_shape</span></code> argument, and in doing so provides the generalized superset implementation among the backend frameworks.</p>
<p><strong>ivy.scatter_nd</strong></p>
<p>When looking at the <code class="xref py py-func docutils literal notranslate"><span class="pre">scatter_nd()</span></code> (or closest equivalent) implementations for <a class="reference external" href="https://unify.ai/docs/ivy/functional/ivy/general/scatter_nd/scatter_nd_functional.html">Ivy</a>, <a class="reference external" href="https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html#jax.numpy.ndarray.at">JAX</a>, <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ufunc.at.html">NumPy</a>, <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/scatter_nd">TensorFlow</a>, and <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.scatter.html">PyTorch</a>, we can see that torch only supports scattering along a single dimension, while all other frameworks support scattering across multiple dimensions at once.
Likewise, Ivy also supports scattering across multiple dimensions at once, and in doing so provides the generalized superset implementation among the backend frameworks.</p>
<p><strong>ivy.logical_and</strong></p>
<p>When looking at the <code class="xref py py-func docutils literal notranslate"><span class="pre">logical_and()</span></code> (or closest equivalent) implementations for <a class="reference external" href="https://unify.ai/docs/ivy/functional/ivy/elementwise/logical_and/logical_and_functional.html">Ivy</a>, <a class="reference external" href="https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.logical_and.html">JAX</a>, <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.logical_and.html">NumPy</a>, <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/math/logical_and">TensorFlow</a>, and <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.logical_and.html">PyTorch</a>, we can see that numpy and torch support the <code class="code docutils literal notranslate"><span class="pre">out</span></code> argument for performing inplace updates, while JAX and tensorflow do not.
With regards to the supported data types, JAX, numpy and torch support numeric arrays, while tensorflow supports only boolean arrays.
With regards to both of these points, Ivy provides the generalized superset implementation among the backend frameworks, with support for the <code class="code docutils literal notranslate"><span class="pre">out</span></code> argument and also support for both numeric and boolean arrays in the input.</p>
<p>However, as discussed above, <code class="xref py py-func docutils literal notranslate"><span class="pre">np.logical_and()</span></code> also supports the <code class="code docutils literal notranslate"><span class="pre">where</span></code> argument, which we opt to <strong>not</strong> support in Ivy.
This is because the behaviour can easily be created as a composition like so <code class="code docutils literal notranslate"><span class="pre">ivy.where(mask,</span> <span class="pre">ivy.logical_and(x,</span> <span class="pre">y),</span> <span class="pre">ivy.zeros_like(mask))</span></code>, and we prioritize the simplicity, clarity, and function uniqueness in Ivy’s API in this case, which comes at the cost of reduced runtime efficiency for some functions when using a NumPy backend.
However, in future releases our automatic graph compilation and graph simplification processes will alleviate these minor inefficiencies entirely from the final computation graph, by fusing multiple operations into one at the API level where possible.</p>
</section>
<section id="maximizing-usage-of-native-functionality">
<h2>Maximizing Usage of Native Functionality<a class="headerlink" href="#maximizing-usage-of-native-functionality" title="Permalink to this heading">¶</a></h2>
<p>While achieving the objective of having superset behaviour across the backends, native functionality of frameworks should be made use of as much as possible.
Even if a framework-specific function doesn’t provide complete superset behaviour, we should still make use of the partial behaviour that it provides and then add more logic for the remaining part.
This is for efficiency reasons and is more explained under the <a class="reference external" href="https://unify.ai/docs/ivy/overview/deep_dive/function_types.html#mixed-functions">Mixed Function</a> section.
In cases when a framework-specific function exists for one or two backends but not the others, we implement a <a class="reference external" href="https://unify.ai/docs/ivy/overview/deep_dive/function_types.html#mixed-functions">Mixed Function</a>.
But when the framework-specific functions do not cover all superset functionality, Ivy also allows for a mixed-compositional hybrid approach.</p>
<p>Consider the example of <code class="xref py py-func docutils literal notranslate"><span class="pre">interpolate()</span></code>.
Most frameworks contain some kind of interpolation function, usually limited to 2D and/or 3D, but <code class="xref py py-func docutils literal notranslate"><span class="pre">ivy.interpolate()</span></code> should be much more general, including interpolations across a larger number of dimensions.
On top of this, different framework-specific functions support different sets of modes for interpolation.
For example, if we look at the framework-specific functions available that serve the purpose of interpolation</p>
<ol class="arabic simple">
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.functional.interpolate()</span></code> supports larger number of dimensions in the input but doesn’t support the <code class="code docutils literal notranslate"><span class="pre">gaussian</span></code> or <code class="code docutils literal notranslate"><span class="pre">mitchellcubic</span></code> modes which are supported by <code class="xref py py-func docutils literal notranslate"><span class="pre">tf.image.resize()</span></code>.</p></li>
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">tf.image.resize()</span></code> supports the <code class="code docutils literal notranslate"><span class="pre">gaussian</span></code> or <code class="code docutils literal notranslate"><span class="pre">mitchellcubic</span></code> modes but doesn’t support some other modes in <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.functional.interpolate()</span></code> and it also doesn’t support larger than a 4-dimensional input.</p></li>
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">jax.image.resize()</span></code> also has missing modes and doesn’t support larger number of dimensions.</p></li>
<li><p><code class="code docutils literal notranslate"><span class="pre">numpy</span></code> doesn’t have an equivalent function for interpolation (<code class="xref py py-func docutils literal notranslate"><span class="pre">numpy.interp()</span></code> is very different from the functionality required).</p></li>
</ol>
<p>So the ideal superset implementation for <code class="xref py py-func docutils literal notranslate"><span class="pre">ivy.interpolate()</span></code> would be supporting the union of all modes supported by different implementations and support a larger number of dimensions in the input.</p>
<p>But there are a few considerations to be made,</p>
<ol class="arabic simple">
<li><p>Implementing all the modes for all the backend-specific implementations would be tedious and repetitive as some modes may not be supported by more than one framework.</p></li>
<li><p>We would need a completely compositional implementation for the <code class="code docutils literal notranslate"><span class="pre">numpy</span></code> backend which doesn’t have an equivalent framework-specific function.</p></li>
<li><p>But also having a single compositional implementation for all backends would be considerably inefficient as compared to the framework-specific functions with overlapping functionality.</p></li>
</ol>
<p>As a workaround, we can simply make use of the backend-specific implementations for a certain number of dimensions and modes for each backend, and then have a general compositional implementation which covers all the remaining cases.
This will make sure that we don’t introduce any inefficiencies and also avoid re-implementation for all the backends.</p>
<p>Ivy allows this using the <a class="reference external" href="https://github.com/unifyai/ivy/blob/840b6fa1dd0ad634d2efc9a4faea30d9404faef9/ivy/functional/backends/torch/experimental/layers.py#L735">partial_mixed_handler</a> attribute on the backend-specific implementation. So the <code class="code docutils literal notranslate"><span class="pre">torch</span></code> backend implementation of <code class="xref py py-func docutils literal notranslate"><span class="pre">interpolate()</span></code> would look like the following,</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">interpolate</span><span class="p">(</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">size</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="nb">int</span><span class="p">],</span>
    <span class="o">/</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">mode</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span>
        <span class="s2">&quot;linear&quot;</span><span class="p">,</span>
        <span class="s2">&quot;bilinear&quot;</span><span class="p">,</span>
        <span class="s2">&quot;trilinear&quot;</span><span class="p">,</span>
        <span class="s2">&quot;nearest&quot;</span><span class="p">,</span>
        <span class="s2">&quot;area&quot;</span><span class="p">,</span>
        <span class="s2">&quot;nearest_exact&quot;</span><span class="p">,</span>
        <span class="s2">&quot;tf_area&quot;</span><span class="p">,</span>
        <span class="s2">&quot;bicubic&quot;</span><span class="p">,</span>
        <span class="s2">&quot;mitchellcubic&quot;</span><span class="p">,</span>
        <span class="s2">&quot;lanczos3&quot;</span><span class="p">,</span>
        <span class="s2">&quot;lanczos5&quot;</span><span class="p">,</span>
        <span class="s2">&quot;gaussian&quot;</span><span class="p">,</span>
    <span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;linear&quot;</span><span class="p">,</span>
    <span class="n">scale_factor</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">recompute_scale_factor</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">align_corners</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">antialias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">out</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">interpolate</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span>
        <span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">,</span>
        <span class="n">mode</span><span class="o">=</span><span class="n">mode</span><span class="p">,</span>
        <span class="n">align_corners</span><span class="o">=</span><span class="n">align_corners</span><span class="p">,</span>
        <span class="n">antialias</span><span class="o">=</span><span class="n">antialias</span><span class="p">,</span>
        <span class="n">scale_factor</span><span class="o">=</span><span class="n">scale_factor</span><span class="p">,</span>
        <span class="n">recompute_scale_factor</span><span class="o">=</span><span class="n">recompute_scale_factor</span><span class="p">,</span>
    <span class="p">)</span>


<span class="n">interpolate</span><span class="o">.</span><span class="n">partial_mixed_handler</span> <span class="o">=</span> <span class="k">lambda</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;linear&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span>
    <span class="s2">&quot;tf_area&quot;</span><span class="p">,</span>
    <span class="s2">&quot;bicubic_tensorflow&quot;</span><span class="p">,</span>
    <span class="s2">&quot;mitchellcubic&quot;</span><span class="p">,</span>
    <span class="s2">&quot;lanczos3&quot;</span><span class="p">,</span>
    <span class="s2">&quot;lanczos5&quot;</span><span class="p">,</span>
    <span class="s2">&quot;gaussian&quot;</span><span class="p">,</span>
<span class="p">]</span>
</pre></div>
</div>
<p>When the backend is set, we use this attribute to apply the <a class="reference external" href="https://github.com/unifyai/ivy/blob/840b6fa1dd0ad634d2efc9a4faea30d9404faef9/ivy/func_wrapper.py#L980">handle_mixed_function</a> decorator to the function.
The <code class="code docutils literal notranslate"><span class="pre">&#64;handle_mixed_function</span></code> accepts a function as an input that receives the arguments and keyword arguments passed to the backend-specific implementation.
The input function is expected to be a boolean function where we’d use the backend-specific implementation if <code class="code docutils literal notranslate"><span class="pre">True</span></code> and the compositional implementation if <code class="code docutils literal notranslate"><span class="pre">False</span></code>.
This provides the flexibility to add any custom logic based on the use-case for maximal use of framework-specific implementations while achieving superset generalization.</p>
<p><strong>Note</strong></p>
<p>Even though we are always striving to adhere to the superset, there might be cases where a feature has slipped under the radar.
In case you stumble upon an Ivy function that you think has not included all native framework functionalities in the optimal way, you are invited to let us know in the comment section of <a class="reference external" href="https://github.com/unifyai/ivy/issues/6406">this</a> dedicated issue.</p>
<p><strong>Round Up</strong></p>
<p>This should have hopefully given you a good feel what should and should not be included when deciding how to design a new Ivy function.
In many cases, there is not a clear right and wrong answer, and we arrive at the final decision via open discussion.
If you find yourself proposing the addition of a new function in Ivy, then we will most likely have this discussion on your Pull Request!</p>
<p>If you have any questions, please feel free to reach out on <a class="reference external" href="https://discord.gg/sXyFF8tDtm">discord</a> in the <a class="reference external" href="https://discord.com/channels/799879767196958751/1018954266322419732">superset behavior channel</a> or in the <a class="reference external" href="https://discord.com/channels/799879767196958751/1028296822386610196">superset behavior forum</a>!</p>
<p><strong>Video</strong></p>
<iframe width="420" height="315"
src="https://www.youtube.com/embed/_D6xER3H4NU" class="video">
</iframe></section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">tr</a></h1>








<h3>Navigation</h3>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2023, re.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 7.0.1</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.13</a>
      
      |
      <a href="_sources/is.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>