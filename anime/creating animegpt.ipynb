{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3125cee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Encoding 'cl100k_base'>\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "\n",
    "# To get the tokeniser corresponding to a specific model in the OpenAI API:\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "print(enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409f2477",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0ac154a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "12c04ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "charachetr length 5370734\n"
     ]
    }
   ],
   "source": [
    "print(\"charachetr length\"  ,len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "76adb5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: A Channel plot summary:  Toeru takes a test so she can enter the same high school as Run, the girl she likes. She passes, but when she goes   to tell Run, she finds her hugging a girl she’s never seen before.\n",
      "name: A Channel Ova  plot summary:  Toeru takes a test so she can enter the same high school as Run, the girl she likes. She passes, but when she goes   to tell Run, she finds her hugging a girl she’s never seen before.\n",
      "name: A Channel Special  plot summary:  Toeru takes a test so she can enter the same high school as Run, the girl she likes. She passes, but when she goes   to tell Run, she finds her hugging a girl she’s never seen before.\n",
      "name: A Day Before Us plot summary:  A Day Before Us is an animated Korean drama of a special and sweet love story of the following four main characters; Yeo Reum, Ha Eun, Kim Wook, and Yeon Woo.\n",
      "name: A Disguised Princess plot summary:  \"Ken always disguised herself as a man. She was totally confused about why she got entangled with that \n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ecf5b40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\n",
      " !\"#$%&'()*+,-./0123456789:;<=?ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz|~ ­°±²´½ÄÈÉÖ×ßàáâãäçèéëíïñóöùüĐēīōūΑΔΨΩθμЯоขงตนบยาเแ่้ấểệ​‐–—―‘’‚“”†…⁠Ⅲ→∞△★☆♡♥♪♭【】いうかじずたつてなのみゃゆよイウォカスダチテナハバミモル・ー一三也井優儿分勝勢原命咒园城好婆婿子季学宝寿山崎幸幼废座式归役意愛我戦書术来柴極武江玉百的福秋科立第絵老舞花袭襲詢講赘車逆道里闘靜養鬼麻龍생역인전﻿！－：＜＞\n",
      "283\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0237578a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(enc.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5361825c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now split up the data into train and validation sets\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "78948435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  609,    25,   362, 13740,  7234, 12399,    25,   220,  2057])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c55ea4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([609]) the target: 25\n",
      "when input is tensor([609,  25]) the target: 362\n",
      "when input is tensor([609,  25, 362]) the target: 13740\n",
      "when input is tensor([  609,    25,   362, 13740]) the target: 7234\n",
      "when input is tensor([  609,    25,   362, 13740,  7234]) the target: 12399\n",
      "when input is tensor([  609,    25,   362, 13740,  7234, 12399]) the target: 25\n",
      "when input is tensor([  609,    25,   362, 13740,  7234, 12399,    25]) the target: 220\n",
      "when input is tensor([  609,    25,   362, 13740,  7234, 12399,    25,   220]) the target: 2057\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2a6fb45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[12399,    25,   220,   362,  8334,  5334,  6532,   304],\n",
      "        [  520,   682,  2853,   627,   609,    25,  1556,    72],\n",
      "        [ 6134,   311,  1461,   627,   609,    25,   452,   347],\n",
      "        [ 1712, 23164,     1,   449,   279,  2410,   311, 37735]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[   25,   220,   362,  8334,  5334,  6532,   304,   459],\n",
      "        [  682,  2853,   627,   609,    25,  1556,    72, 13080],\n",
      "        [  311,  1461,   627,   609,    25,   452,   347,   373],\n",
      "        [23164,     1,   449,   279,  2410,   311, 37735,   892]])\n",
      "----\n",
      "when input is [12399] the target: 25\n",
      "when input is [12399, 25] the target: 220\n",
      "when input is [12399, 25, 220] the target: 362\n",
      "when input is [12399, 25, 220, 362] the target: 8334\n",
      "when input is [12399, 25, 220, 362, 8334] the target: 5334\n",
      "when input is [12399, 25, 220, 362, 8334, 5334] the target: 6532\n",
      "when input is [12399, 25, 220, 362, 8334, 5334, 6532] the target: 304\n",
      "when input is [12399, 25, 220, 362, 8334, 5334, 6532, 304] the target: 459\n",
      "when input is [520] the target: 682\n",
      "when input is [520, 682] the target: 2853\n",
      "when input is [520, 682, 2853] the target: 627\n",
      "when input is [520, 682, 2853, 627] the target: 609\n",
      "when input is [520, 682, 2853, 627, 609] the target: 25\n",
      "when input is [520, 682, 2853, 627, 609, 25] the target: 1556\n",
      "when input is [520, 682, 2853, 627, 609, 25, 1556] the target: 72\n",
      "when input is [520, 682, 2853, 627, 609, 25, 1556, 72] the target: 13080\n",
      "when input is [6134] the target: 311\n",
      "when input is [6134, 311] the target: 1461\n",
      "when input is [6134, 311, 1461] the target: 627\n",
      "when input is [6134, 311, 1461, 627] the target: 609\n",
      "when input is [6134, 311, 1461, 627, 609] the target: 25\n",
      "when input is [6134, 311, 1461, 627, 609, 25] the target: 452\n",
      "when input is [6134, 311, 1461, 627, 609, 25, 452] the target: 347\n",
      "when input is [6134, 311, 1461, 627, 609, 25, 452, 347] the target: 373\n",
      "when input is [1712] the target: 23164\n",
      "when input is [1712, 23164] the target: 1\n",
      "when input is [1712, 23164, 1] the target: 449\n",
      "when input is [1712, 23164, 1, 449] the target: 279\n",
      "when input is [1712, 23164, 1, 449, 279] the target: 2410\n",
      "when input is [1712, 23164, 1, 449, 279, 2410] the target: 311\n",
      "when input is [1712, 23164, 1, 449, 279, 2410, 311] the target: 37735\n",
      "when input is [1712, 23164, 1, 449, 279, 2410, 311, 37735] the target: 892\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3a27027f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.nn import functional as F\n",
    "# torch.manual_seed(1337)\n",
    "# class BigramLanguangeModel(nn.Module):\n",
    "#     def __init__(self, vocab_size):\n",
    "#         super().__init__()\n",
    "#         self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "#     def forward(self, idx, targets):\n",
    "#         logits = self.token_embedding_table(idx)\n",
    "        \n",
    "        \n",
    "#         loss = F.cross_entropy(logits, targets)\n",
    "#         return logits\n",
    "    \n",
    "# m = BigramLanguangeModel(vocab_size)\n",
    "# logits = m(xb, yb)\n",
    "# print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "327218d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx shape: torch.Size([3]), type: torch.int64\n",
      "targets shape: torch.Size([3]), type: torch.int64\n",
      "xb shape: torch.Size([4, 8]), type: torch.int64\n",
      "yb shape: torch.Size([4, 8]), type: torch.int64\n",
      "vocab_size 283\n",
      "Embedding table size: torch.Size([283, 283])\n",
      "logits shape: torch.Size([3, 283])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[118], line 45\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Debugging Step 4: Check the shape of the logits tensor\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogits\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 45\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[118], line 13\u001b[0m, in \u001b[0;36mBigramLanguageModel.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx, targets):\n\u001b[0;32m---> 13\u001b[0m         logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken_embedding_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#         B,T,C = logits.shape\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#         logits= logits.view(B*T, C)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#         targets = targets.view(B*T)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m         loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logits, targets)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets):\n",
    "        logits = self.token_embedding_table(idx)\n",
    "#         B,T,C = logits.shape\n",
    "#         logits= logits.view(B*T, C)\n",
    "#         targets = targets.view(B*T)\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        return logits  \n",
    "    \n",
    "# Define the vocab_size, idx, targets, xb, and yb\n",
    "# vocab_size = 100\n",
    "# idx = torch.tensor([1, 2, 3])\n",
    "# targets = torch.tensor([0, 1, 0])\n",
    "# xb = torch.tensor([1, 2, 3])\n",
    "# yb = torch.tensor([0, 1, 0])\n",
    "\n",
    "# Create an instance of the BigramLanguageModel\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "\n",
    "# Debugging Step 1: Verify the shapes and data types of variables\n",
    "print(f\"idx shape: {idx.shape}, type: {idx.dtype}\")\n",
    "print(f\"targets shape: {targets.shape}, type: {targets.dtype}\")\n",
    "print(f\"xb shape: {xb.shape}, type: {xb.dtype}\")\n",
    "print(f\"yb shape: {yb.shape}, type: {yb.dtype}\")\n",
    "print(\"vocab_size\", vocab_size)\n",
    "\n",
    "# Debugging Step 2: Check the size of the embedding table\n",
    "print(f\"Embedding table size: {m.token_embedding_table.weight.size()}\")\n",
    "\n",
    "# Debugging Step 3: Verify the compatibility of input tensors with the model architecture\n",
    "logits = m(idx, targets)\n",
    "\n",
    "# Debugging Step 4: Check the shape of the logits tensor\n",
    "print(f\"logits shape: {logits.shape}\")\n",
    "logits = m(xb, yb)\n",
    "# print(logits.shape)\n",
    "# print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7fd4f3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 100])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cfa0fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
